/**
 * AI Vulnerability Protection Module
 * Comprehensive protection against AI/Chat-specific security vulnerabilities
 */

import { getSecurityCore } from './SecurityCore';
import { SecurityUtils } from './SecurityUtils';
import { ConciergusOpenTelemetry } from '../telemetry/OpenTelemetryConfig';

/**
 * Content filter levels
 */
export enum ContentFilterLevel {
  PERMISSIVE = 'permissive',
  MODERATE = 'moderate',
  STRICT = 'strict',
  ENTERPRISE = 'enterprise'
}

/**
 * Threat categories for AI interactions
 */
export enum AIThreatCategory {
  PROMPT_INJECTION = 'prompt_injection',
  DATA_LEAKAGE = 'data_leakage',
  HARMFUL_CONTENT = 'harmful_content',
  MANIPULATION_ATTEMPT = 'manipulation_attempt',
  JAILBREAK_ATTEMPT = 'jailbreak_attempt',
  SYSTEM_DISCLOSURE = 'system_disclosure',
  SENSITIVE_INFO_REQUEST = 'sensitive_info_request'
}

/**
 * AI threat assessment result
 */
export interface AIThreatAssessment {
  threatDetected: boolean;
  threatCategory?: AIThreatCategory;
  severity: 'low' | 'medium' | 'high' | 'critical';
  confidence: number; // 0-1
  patterns: string[];
  recommendation: 'allow' | 'sanitize' | 'block' | 'log_and_allow';
  sanitizedContent?: string;
  metadata: {
    detectionTime: number;
    rulesTriggered: string[];
    riskScore: number; // 0-100
  };
}

/**
 * Content filtering result
 */
export interface ContentFilterResult {
  safe: boolean;
  filtered?: string;
  violations: {
    category: string;
    severity: 'low' | 'medium' | 'high' | 'critical';
    description: string;
    position?: { start: number; end: number };
  }[];
  metadata: {
    filterLevel: ContentFilterLevel;
    processingTime: number;
    wordsFiltered: number;
  };
}

/**
 * Data leakage protection result
 */
export interface DataLeakageAssessment {
  riskDetected: boolean;
  riskLevel: 'low' | 'medium' | 'high' | 'critical';
  patterns: string[];
  recommendations: string[];
  redactedContent?: string;
  metadata: {
    sensitiveDataTypes: string[];
    confidenceScore: number;
    redactionCount: number;
  };
}

/**
 * Comprehensive prompt injection patterns
 */
const ADVANCED_INJECTION_PATTERNS = [
  // Direct instruction override
  /ignore\s+(all\s+)?previous\s+(instructions?|prompts?|rules?|context)/gi,
  /forget\s+(everything|all|what\s+you\s+learned|previous|instructions?)/gi,
  /disregard\s+(previous|all|above|prior)\s+(instructions?|context|conversation)/gi,
  /override\s+(instructions?|settings?|rules?|parameters?|defaults?)/gi,
  
  // Role manipulation
  /system\s*:\s*(you\s+are\s+now|respond\s+as|act\s+as)/gi,
  /new\s+(instructions?|task|role|persona|character|identity)/gi,
  /respond\s+as\s+(if\s+you\s+are|a\s+different|someone\s+else)/gi,
  /pretend\s+(to\s+be|you\s+are|that\s+you)/gi,
  /(roleplay|role-play)\s+as\s+(a\s+)?([^.\n]{1,50})/gi,
  
  // System prompts and delimiters
  /\[\s*system\s*\]/gi,
  /\{\s*system\s*\}/gi,
  /<\s*system\s*>/gi,
  /SYSTEM\s*:/gi,
  /USER\s*:/gi,
  /ASSISTANT\s*:/gi,
  /\*\*\*\s*(system|admin|root|debug)\s*\*\*\*/gi,
  
  // Jailbreak attempts
  /jailbreak\s*[\:\-\|]/gi,
  /developer\s+mode/gi,
  /debug\s+mode/gi,
  /(break|bypass|circumvent|escape)\s+(safety|guidelines?|restrictions?|filters?)/gi,
  
  // Instruction leakage attempts
  /(show|display|reveal|tell\s+me)\s+(your\s+)?(instructions?|prompt|system\s+prompt)/gi,
  /what\s+(are\s+your|is\s+your)\s+(instructions?|prompt|system\s+message)/gi,
  /(copy|repeat|echo)\s+(your\s+)?(initial|original|system)\s+(prompt|instructions?)/gi,
  
  // Code injection
  /```\s*(python|javascript|bash|shell|cmd)/gi,
  /<script[^>]*>/gi,
  /eval\s*\(/gi,
  /exec\s*\(/gi,
  
  // Social engineering
  /for\s+(educational|research|testing)\s+purposes?\s+only/gi,
  /this\s+is\s+(urgent|important|critical)/gi,
  /my\s+(boss|supervisor|teacher)\s+(told|asked|said)/gi,
  
  // Encoding attempts
  /base64\s*:/gi,
  /rot13\s*:/gi,
  /\\u[0-9a-f]{4}/gi,
  /%[0-9a-f]{2}/gi,
];

/**
 * Harmful content patterns
 */
const HARMFUL_CONTENT_PATTERNS = {
  violence: [
    /\b(kill|murder|assassinate|harm|hurt|attack|assault|violence|weapon|bomb|explosive)\b/gi,
    /\b(stab|shoot|poison|torture|abuse|threaten)\b/gi,
  ],
  hate_speech: [
    /\b(hate|racist|nazi|supremacist|discrimination)\b/gi,
    /\b(terrorist|extremist|radical)\b/gi,
  ],
  illegal_activities: [
    /\b(drugs|illegal|fraud|scam|hack|piracy|theft|robbery)\b/gi,
    /\b(counterfeit|smuggle|launder|bribe)\b/gi,
  ],
  self_harm: [
    /\b(suicide|self\s*harm|cut\s+myself|end\s+my\s+life)\b/gi,
    /\b(overdose|hanging|jumping)\b/gi,
  ],
  sexual_content: [
    /\b(sexual|explicit|pornographic|adult\s+content)\b/gi,
  ],
  personal_info: [
    /\b(social\s+security|ssn|credit\s+card|password|login)\b/gi,
    /\b(\d{3}-\d{2}-\d{4}|\d{4}\s+\d{4}\s+\d{4}\s+\d{4})\b/gi,
  ]
};

/**
 * Data leakage patterns
 */
const DATA_LEAKAGE_PATTERNS = [
  // API keys and tokens
  /\b[A-Za-z0-9]{32,}\b/g, // Generic API keys
  /sk-[A-Za-z0-9]{48}/g, // OpenAI API keys
  /xoxb-[A-Za-z0-9-]+/g, // Slack tokens
  /ghp_[A-Za-z0-9]{36}/g, // GitHub tokens
  
  // Personal identifiers
  /\b\d{3}-\d{2}-\d{4}\b/g, // SSN
  /\b\d{4}\s+\d{4}\s+\d{4}\s+\d{4}\b/g, // Credit cards
  /\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b/g, // Emails
  
  // System information
  /\b(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\b/g, // IP addresses
  /\b(?:[a-zA-Z0-9](?:[a-zA-Z0-9\-]{0,61}[a-zA-Z0-9])?\.)+[a-zA-Z]{2,}\b/g, // Domain names
  
  // File paths
  /[C-Z]:\\(?:[^\\/:*?"<>|\r\n]+\\)*[^\\/:*?"<>|\r\n]*/g, // Windows paths
  /\/(?:[^\/\s]+\/)*[^\/\s]+/g, // Unix paths
];

/**
 * AI Vulnerability Protection Engine
 */
export class AIVulnerabilityProtection {
  private static instance: AIVulnerabilityProtection | null = null;
  private securityCore = getSecurityCore();

  private constructor() {}

  static getInstance(): AIVulnerabilityProtection {
    if (!AIVulnerabilityProtection.instance) {
      AIVulnerabilityProtection.instance = new AIVulnerabilityProtection();
    }
    return AIVulnerabilityProtection.instance;
  }

  /**
   * Comprehensive AI threat assessment
   */
  async assessAIThreat(
    content: string,
    context?: {
      source: 'user_input' | 'ai_output' | 'system_prompt';
      userId?: string;
      sessionId?: string;
      conversationId?: string;
    }
  ): Promise<AIThreatAssessment> {
    const startTime = performance.now();
    const config = this.securityCore.getConfig();

    if (!config.aiSecurity.enableInjectionProtection) {
      return {
        threatDetected: false,
        severity: 'low',
        confidence: 0,
        patterns: [],
        recommendation: 'allow',
        metadata: {
          detectionTime: performance.now() - startTime,
          rulesTriggered: [],
          riskScore: 0
        }
      };
    }

    const rulesTriggered: string[] = [];
    const patterns: string[] = [];
    let riskScore = 0;
    let threatCategory: AIThreatCategory | undefined;
    let maxSeverity: 'low' | 'medium' | 'high' | 'critical' = 'low';

    // Check for prompt injection
    const injectionCheck = this.detectPromptInjection(content);
    if (injectionCheck.detected) {
      threatCategory = AIThreatCategory.PROMPT_INJECTION;
      patterns.push(...injectionCheck.patterns);
      rulesTriggered.push('prompt_injection');
      riskScore += injectionCheck.severity === 'critical' ? 40 : 
                   injectionCheck.severity === 'high' ? 30 :
                   injectionCheck.severity === 'medium' ? 20 : 10;
      if (injectionCheck.severity === 'critical') maxSeverity = 'critical';
      else if (injectionCheck.severity === 'high' && maxSeverity !== 'critical') maxSeverity = 'high';
      else if (injectionCheck.severity === 'medium' && maxSeverity === 'low') maxSeverity = 'medium';
    }

    // Check for jailbreak attempts
    const jailbreakCheck = this.detectJailbreakAttempt(content);
    if (jailbreakCheck.detected) {
      if (!threatCategory) threatCategory = AIThreatCategory.JAILBREAK_ATTEMPT;
      patterns.push(...jailbreakCheck.patterns);
      rulesTriggered.push('jailbreak_attempt');
      riskScore += 35;
      if (maxSeverity !== 'critical') maxSeverity = 'high';
    }

    // Check for system disclosure attempts
    const systemDisclosureCheck = this.detectSystemDisclosure(content);
    if (systemDisclosureCheck.detected) {
      if (!threatCategory) threatCategory = AIThreatCategory.SYSTEM_DISCLOSURE;
      patterns.push(...systemDisclosureCheck.patterns);
      rulesTriggered.push('system_disclosure');
      riskScore += 25;
      if (maxSeverity === 'low') maxSeverity = 'medium';
    }

    // Check for manipulation attempts
    const manipulationCheck = this.detectManipulationAttempt(content);
    if (manipulationCheck.detected) {
      if (!threatCategory) threatCategory = AIThreatCategory.MANIPULATION_ATTEMPT;
      patterns.push(...manipulationCheck.patterns);
      rulesTriggered.push('manipulation_attempt');
      riskScore += 20;
      if (maxSeverity === 'low') maxSeverity = 'medium';
    }

    // Normalize risk score and calculate confidence
    riskScore = Math.min(100, riskScore);
    const confidence = Math.min(1, riskScore / 100);
    const threatDetected = riskScore > 15; // Threshold for threat detection

    // Determine recommendation
    let recommendation: AIThreatAssessment['recommendation'] = 'allow';
    if (riskScore >= 80) recommendation = 'block';
    else if (riskScore >= 50) recommendation = 'sanitize';
    else if (riskScore >= 20) recommendation = 'log_and_allow';

    const detectionTime = performance.now() - startTime;

    // Create sanitized content if needed
    let sanitizedContent: string | undefined;
    if (recommendation === 'sanitize') {
      sanitizedContent = this.sanitizeAIContent(content);
    }

    // Log threat detection
    if (threatDetected) {
      this.logThreatDetection({
        threatCategory,
        severity: maxSeverity,
        confidence,
        riskScore,
        patterns,
        context,
        content: content.substring(0, 200) // Log first 200 chars only
      });
    }

    const result: AIThreatAssessment = {
      threatDetected,
      severity: maxSeverity,
      confidence,
      patterns,
      recommendation,
      metadata: {
        detectionTime,
        rulesTriggered,
        riskScore
      }
    };

    // Only add optional properties if they have values
    if (threatCategory) {
      result.threatCategory = threatCategory;
    }
    if (sanitizedContent) {
      result.sanitizedContent = sanitizedContent;
    }

    return result;
  }

  /**
   * Content filtering for harmful outputs
   */
  async filterContent(
    content: string,
    filterLevel: ContentFilterLevel = ContentFilterLevel.MODERATE
  ): Promise<ContentFilterResult> {
    const startTime = performance.now();
    const config = this.securityCore.getConfig();

    if (!config.aiSecurity.enableContentFiltering) {
      return {
        safe: true,
        violations: [],
        metadata: {
          filterLevel,
          processingTime: performance.now() - startTime,
          wordsFiltered: 0
        }
      };
    }

    const violations: ContentFilterResult['violations'] = [];
    let filtered = content;
    let wordsFiltered = 0;

    // Check each harmful content category
    for (const [category, patterns] of Object.entries(HARMFUL_CONTENT_PATTERNS)) {
      for (const pattern of patterns) {
        const matches = content.match(pattern);
        if (matches) {
          for (const match of matches) {
            const severity = this.getViolationSeverity(category, filterLevel);
            violations.push({
              category,
              severity,
              description: `Detected ${category.replace('_', ' ')} content: "${match}"`,
              position: {
                start: content.indexOf(match),
                end: content.indexOf(match) + match.length
              }
            });

            // Filter based on level
            if (this.shouldFilterContent(category, filterLevel)) {
              filtered = filtered.replace(pattern, '[FILTERED]');
              wordsFiltered++;
            }
          }
        }
      }
    }

    const processingTime = performance.now() - startTime;
    const safe = violations.length === 0 || !violations.some(v => v.severity === 'high' || v.severity === 'critical');

    const result: ContentFilterResult = {
      safe,
      violations,
      metadata: {
        filterLevel,
        processingTime,
        wordsFiltered
      }
    };

    // Only add filtered property if content was actually filtered
    if (filtered !== content) {
      result.filtered = filtered;
    }

    return result;
  }

  /**
   * Data leakage prevention assessment
   */
  async assessDataLeakage(
    content: string,
    context?: {
      contentType: 'input' | 'output';
      userId?: string;
      sensitivityLevel?: 'low' | 'medium' | 'high' | 'critical';
    }
  ): Promise<DataLeakageAssessment> {
    const patterns: string[] = [];
    const sensitiveDataTypes: string[] = [];
    const recommendations: string[] = [];
    let redactedContent = content;
    let redactionCount = 0;
    let confidenceScore = 0;

    // Check for various types of sensitive data
    for (const [index, pattern] of DATA_LEAKAGE_PATTERNS.entries()) {
      const matches = content.match(pattern);
      if (matches) {
        for (const match of matches) {
          patterns.push(match);
          redactedContent = redactedContent.replace(match, '[REDACTED]');
          redactionCount++;
          
          // Categorize the sensitive data type
          if (index <= 3) {
            sensitiveDataTypes.push('api_keys');
            confidenceScore += 0.3;
          } else if (index <= 6) {
            sensitiveDataTypes.push('personal_identifiers');
            confidenceScore += 0.25;
          } else if (index <= 8) {
            sensitiveDataTypes.push('system_information');
            confidenceScore += 0.15;
          } else {
            sensitiveDataTypes.push('file_paths');
            confidenceScore += 0.1;
          }
        }
      }
    }

    // Generate recommendations
    if (sensitiveDataTypes.includes('api_keys')) {
      recommendations.push('Rotate exposed API keys immediately');
    }
    if (sensitiveDataTypes.includes('personal_identifiers')) {
      recommendations.push('Review data handling practices for PII');
    }
    if (sensitiveDataTypes.includes('system_information')) {
      recommendations.push('Audit system information exposure');
    }

    const riskLevel: DataLeakageAssessment['riskLevel'] = 
      confidenceScore >= 0.8 ? 'critical' :
      confidenceScore >= 0.6 ? 'high' :
      confidenceScore >= 0.3 ? 'medium' : 'low';

    const result: DataLeakageAssessment = {
      riskDetected: patterns.length > 0,
      riskLevel,
      patterns,
      recommendations,
      metadata: {
        sensitiveDataTypes: [...new Set(sensitiveDataTypes)],
        confidenceScore: Math.min(1, confidenceScore),
        redactionCount
      }
    };

    // Only add redacted content if redaction occurred
    if (redactionCount > 0) {
      result.redactedContent = redactedContent;
    }

    return result;
  }

  /**
   * Sanitize AI content while preserving meaning
   */
  sanitizeAIContent(content: string): string {
    let sanitized = content;

    // Remove obvious injection attempts
    ADVANCED_INJECTION_PATTERNS.forEach(pattern => {
      sanitized = sanitized.replace(pattern, '[filtered]');
    });

    // Apply general sanitization
    sanitized = SecurityUtils.sanitizeInput(sanitized);

    return sanitized;
  }

  /**
   * Detect prompt injection attempts
   */
  private detectPromptInjection(content: string): {
    detected: boolean;
    patterns: string[];
    severity: 'low' | 'medium' | 'high' | 'critical';
  } {
    const patterns: string[] = [];
    let maxSeverity: 'low' | 'medium' | 'high' | 'critical' = 'low';

    for (const pattern of ADVANCED_INJECTION_PATTERNS) {
      const matches = content.match(pattern);
      if (matches) {
        patterns.push(...matches);
        
        // Determine severity based on pattern type
        const patternStr = pattern.source;
        if (patternStr.includes('system') || patternStr.includes('SYSTEM')) {
          maxSeverity = 'critical';
        } else if (patternStr.includes('ignore') || patternStr.includes('forget')) {
          if (maxSeverity !== 'critical') maxSeverity = 'high';
        } else if (patternStr.includes('jailbreak') || patternStr.includes('override')) {
          if (maxSeverity === 'low') maxSeverity = 'medium';
        }
      }
    }

    return {
      detected: patterns.length > 0,
      patterns,
      severity: maxSeverity
    };
  }

  /**
   * Detect jailbreak attempts
   */
  private detectJailbreakAttempt(content: string): {
    detected: boolean;
    patterns: string[];
  } {
    const jailbreakPatterns = [
      /jailbreak/gi,
      /break\s+(out\s+of\s+)?character/gi,
      /developer\s+mode/gi,
      /debug\s+mode/gi,
      /sudo\s+mode/gi,
      /admin\s+mode/gi,
      /bypass\s+(safety|filter|restriction)/gi,
      /circumvent\s+(guideline|rule|policy)/gi
    ];

    const patterns: string[] = [];
    for (const pattern of jailbreakPatterns) {
      const matches = content.match(pattern);
      if (matches) {
        patterns.push(...matches);
      }
    }

    return {
      detected: patterns.length > 0,
      patterns
    };
  }

  /**
   * Detect system disclosure attempts
   */
  private detectSystemDisclosure(content: string): {
    detected: boolean;
    patterns: string[];
  } {
    const disclosurePatterns = [
      /(show|tell|reveal|display)\s+(me\s+)?(your\s+)?(initial|original|system)\s+(prompt|instruction)/gi,
      /(what|how)\s+(is|are)\s+(your\s+)?(instruction|prompt|rule)/gi,
      /copy\s+(and\s+paste\s+)?(your\s+)?(system\s+)?(prompt|instruction)/gi,
      /repeat\s+(your\s+)?(initial|original|system)\s+(message|prompt)/gi
    ];

    const patterns: string[] = [];
    for (const pattern of disclosurePatterns) {
      const matches = content.match(pattern);
      if (matches) {
        patterns.push(...matches);
      }
    }

    return {
      detected: patterns.length > 0,
      patterns
    };
  }

  /**
   * Detect manipulation attempts
   */
  private detectManipulationAttempt(content: string): {
    detected: boolean;
    patterns: string[];
  } {
    const manipulationPatterns = [
      /for\s+(educational|research|academic)\s+purposes?\s+only/gi,
      /this\s+is\s+(urgent|emergency|critical|important)/gi,
      /my\s+(boss|teacher|professor|supervisor)\s+(said|told|asked)/gi,
      /I\s+(need|have)\s+to\s+(pass|complete)\s+(test|assignment|homework)/gi,
      /hypothetically?\s+speaking/gi,
      /in\s+a\s+(fictional|hypothetical)\s+(world|scenario|universe)/gi
    ];

    const patterns: string[] = [];
    for (const pattern of manipulationPatterns) {
      const matches = content.match(pattern);
      if (matches) {
        patterns.push(...matches);
      }
    }

    return {
      detected: patterns.length > 0,
      patterns
    };
  }

  /**
   * Get violation severity based on content category and filter level
   */
  private getViolationSeverity(
    category: string,
    filterLevel: ContentFilterLevel
  ): 'low' | 'medium' | 'high' | 'critical' {
    const severityMatrix: Record<string, Record<ContentFilterLevel, 'low' | 'medium' | 'high' | 'critical'>> = {
      violence: {
        [ContentFilterLevel.PERMISSIVE]: 'low',
        [ContentFilterLevel.MODERATE]: 'medium',
        [ContentFilterLevel.STRICT]: 'high',
        [ContentFilterLevel.ENTERPRISE]: 'critical'
      },
      hate_speech: {
        [ContentFilterLevel.PERMISSIVE]: 'medium',
        [ContentFilterLevel.MODERATE]: 'high',
        [ContentFilterLevel.STRICT]: 'critical',
        [ContentFilterLevel.ENTERPRISE]: 'critical'
      },
      illegal_activities: {
        [ContentFilterLevel.PERMISSIVE]: 'medium',
        [ContentFilterLevel.MODERATE]: 'high',
        [ContentFilterLevel.STRICT]: 'critical',
        [ContentFilterLevel.ENTERPRISE]: 'critical'
      },
      self_harm: {
        [ContentFilterLevel.PERMISSIVE]: 'high',
        [ContentFilterLevel.MODERATE]: 'critical',
        [ContentFilterLevel.STRICT]: 'critical',
        [ContentFilterLevel.ENTERPRISE]: 'critical'
      },
      sexual_content: {
        [ContentFilterLevel.PERMISSIVE]: 'low',
        [ContentFilterLevel.MODERATE]: 'medium',
        [ContentFilterLevel.STRICT]: 'high',
        [ContentFilterLevel.ENTERPRISE]: 'high'
      },
      personal_info: {
        [ContentFilterLevel.PERMISSIVE]: 'medium',
        [ContentFilterLevel.MODERATE]: 'high',
        [ContentFilterLevel.STRICT]: 'critical',
        [ContentFilterLevel.ENTERPRISE]: 'critical'
      }
    };

    return severityMatrix[category]?.[filterLevel] || 'low';
  }

  /**
   * Determine if content should be filtered based on category and level
   */
  private shouldFilterContent(category: string, filterLevel: ContentFilterLevel): boolean {
    const filterMatrix: Record<string, ContentFilterLevel[]> = {
      violence: [ContentFilterLevel.STRICT, ContentFilterLevel.ENTERPRISE],
      hate_speech: [ContentFilterLevel.MODERATE, ContentFilterLevel.STRICT, ContentFilterLevel.ENTERPRISE],
      illegal_activities: [ContentFilterLevel.MODERATE, ContentFilterLevel.STRICT, ContentFilterLevel.ENTERPRISE],
      self_harm: [ContentFilterLevel.PERMISSIVE, ContentFilterLevel.MODERATE, ContentFilterLevel.STRICT, ContentFilterLevel.ENTERPRISE],
      sexual_content: [ContentFilterLevel.STRICT, ContentFilterLevel.ENTERPRISE],
      personal_info: [ContentFilterLevel.MODERATE, ContentFilterLevel.STRICT, ContentFilterLevel.ENTERPRISE]
    };

    return filterMatrix[category]?.includes(filterLevel) || false;
  }

  /**
   * Log threat detection for monitoring
   */
  private logThreatDetection(details: {
    threatCategory?: AIThreatCategory;
    severity: string;
    confidence: number;
    riskScore: number;
    patterns: string[];
    context?: any;
    content: string;
  }): void {
    ConciergusOpenTelemetry.createSpan(
      'conciergus-security',
      'ai-threat-detected',
      (span) => {
        span?.setAttributes({
          'threat.category': details.threatCategory || 'unknown',
          'threat.severity': details.severity,
          'threat.confidence': details.confidence,
          'threat.risk_score': details.riskScore,
          'threat.patterns_count': details.patterns.length,
          'threat.content_length': details.content.length,
          'threat.user_id': details.context?.userId || 'anonymous',
          'threat.session_id': details.context?.sessionId || 'unknown'
        });
      }
    );

    ConciergusOpenTelemetry.recordMetric(
      'conciergus-security',
      'ai_threats_detected',
      1,
      {
        category: details.threatCategory || 'unknown',
        severity: details.severity,
        source: details.context?.source || 'unknown'
      }
    );
  }
}

// Export singleton instance
export const aiVulnerabilityProtection = AIVulnerabilityProtection.getInstance();
